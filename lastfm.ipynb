{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heard-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hundred-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    totalUsers  totalPlays     avgPlays\n",
      "name                                                   \n",
      "Britney Spears             522     2393140  4584.559387\n",
      "Depeche Mode               282     1301308  4614.567376\n",
      "Lady Gaga                  611     1291387  2113.563011\n",
      "Christina Aguilera         407     1058405  2600.503686\n",
      "Paramore                   399      963449  2414.659148\n",
      "...                        ...         ...          ...\n",
      "Morris                       1           1     1.000000\n",
      "Eddie Kendricks              1           1     1.000000\n",
      "Excess Pressure              1           1     1.000000\n",
      "My Mine                      1           1     1.000000\n",
      "A.M. Architect               1           1     1.000000\n",
      "\n",
      "[17632 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "plays = pd.read_csv('dataset/user_artists.dat', sep='\\t')\n",
    "artists = pd.read_csv('dataset/artists.dat', sep='\\t', usecols=['id','name'])\n",
    "\n",
    "# Merge artist and user pref data\n",
    "ap = pd.merge(artists, plays, how=\"inner\", left_on=\"id\", right_on=\"artistID\")\n",
    "ap = ap.rename(columns={\"weight\": \"playCount\"})\n",
    "\n",
    "# Group artist by name\n",
    "artist_rank = ap.groupby(['name']) \\\n",
    "    .agg({'userID' : 'count', 'playCount' : 'sum'}) \\\n",
    "    .rename(columns={\"userID\" : 'totalUsers', \"playCount\" : \"totalPlays\"}) \\\n",
    "    .sort_values(['totalPlays'], ascending=False)\n",
    "\n",
    "artist_rank['avgPlays'] = artist_rank['totalPlays'] / artist_rank['totalUsers']\n",
    "print(artist_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moving-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.28\n"
     ]
    }
   ],
   "source": [
    "# Merge into ap matrix\n",
    "ap = ap.join(artist_rank, on=\"name\", how=\"inner\") \\\n",
    "    .sort_values(['playCount'], ascending=False)\n",
    "\n",
    "# Preprocessing\n",
    "pc = ap.playCount\n",
    "play_count_scaled = (pc - pc.min()) / (pc.max() - pc.min())\n",
    "ap = ap.assign(playCountScaled=play_count_scaled)\n",
    "#print(ap)\n",
    "\n",
    "# Build a user-artist rating matrix \n",
    "ratings_df = ap.pivot(index='userID', columns='artistID', values='playCountScaled')\n",
    "ratings = ratings_df.fillna(0).values\n",
    "\n",
    "# Show sparsity\n",
    "sparsity = float(len(ratings.nonzero()[0])) / (ratings.shape[0] * ratings.shape[1]) * 100\n",
    "print(\"sparsity: %.2f\" % sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "specified-arlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating matrix shape (1892, 17632)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Build a sparse matrix\n",
    "X = csr_matrix(ratings)\n",
    "\n",
    "n_users, n_items = ratings_df.shape\n",
    "print(\"rating matrix shape\", ratings_df.shape)\n",
    "\n",
    "user_ids = ratings_df.index.values\n",
    "artist_names = ap.sort_values(\"artistID\")[\"name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "imported-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1892, 17632)\n",
      "Xcoo: (1892, 17632)\n"
     ]
    }
   ],
   "source": [
    "### from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score, precision_at_k, recall_at_k\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "# Build data references + train test\n",
    "print(\"X:\", X.shape)\n",
    "Xcoo = X.tocoo()\n",
    "print(\"Xcoo:\", Xcoo.shape)\n",
    "\n",
    "data = Dataset()\n",
    "data.fit(np.arange(n_users), np.arange(n_items))\n",
    "interactions, weights = data.build_interactions(zip(Xcoo.row, Xcoo.col, Xcoo.data)) \n",
    "train, test = random_train_test_split(interactions)\n",
    "\n",
    "# Ignore that (weight seems to be ignored...)\n",
    "#train = train_.tocsr()\n",
    "#test = test_.tocsr()\n",
    "#train[train==1] = X[train==1]\n",
    "#test[test==1] = X[test==1]\n",
    "\n",
    "# To be completed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-indie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "union-radio",
   "metadata": {},
   "source": [
    "* ### WARP (Weighted Approximate-Rank Pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funny-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f6c4be66ca0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='warp')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-mississippi",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sublime-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.39, test 0.14.\n",
      "AUC: train 0.97, test 0.86.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-dining",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "transparent-rendering",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-basics",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-square",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hungarian-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Depeche Mode' 'Pet Shop Boys' 'Duran Duran' ... 'Avril Lavigne'\n",
      " 'Miley Cyrus' 'Rihanna']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_warp = model.predict(0, np.arange(n_items))\n",
    "top_items_warp = artist_names[np.argsort(-scores_warp)]\n",
    "print(top_items_warp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-circuit",
   "metadata": {},
   "source": [
    "* ### BPR (Bayesian Personalised Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-share",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f6c43658a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='bpr')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-commonwealth",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bridal-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.36, test 0.12.\n",
      "AUC: train 0.84, test 0.78.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-aaron",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bored-equality",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-colon",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "isolated-anime",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Lady Gaga' 'Britney Spears' 'The Beatles' ... 'Planningtorock'\n",
      " 'So Long Forgotten' 'Herman Düne']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Pet Shop Boys' 'Duran Duran' ... 'Avril Lavigne'\n",
      " 'Miley Cyrus' 'Rihanna']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_bpr = model.predict(0, np.arange(n_items))\n",
    "top_items_bpr = artist_names[np.argsort(-scores_bpr)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ranking-graham",
   "metadata": {},
   "source": [
    "print(\"user %s\" % user_id)\n",
    "print(\"     Known positives:\")\n",
    "for x in known_positives[:3]:\n",
    "    print(\"     %s\" % x)\n",
    "\n",
    "print(\" Recommended:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-harris",
   "metadata": {},
   "source": [
    "* ### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lovely-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f6c436583a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='logistic')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-copper",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "earlier-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.20, test 0.07.\n",
      "AUC: train 0.89, test 0.81.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-slope",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "perceived-malawi",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-statement",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "approved-guinea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Lady Gaga' 'Britney Spears' 'The Beatles' ... 'Planningtorock'\n",
      " 'So Long Forgotten' 'Herman Düne']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Pet Shop Boys' 'Duran Duran' ... 'Avril Lavigne'\n",
      " 'Miley Cyrus' 'Rihanna']\n",
      "Top items Logistic: \n",
      " ['Lady Gaga' 'Britney Spears' 'Rihanna' ... 'The Lively Ones'\n",
      " 'Dark Lunacy' 'Bride']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_log = model.predict(0, np.arange(n_items))\n",
    "top_items_log = artist_names[np.argsort(-scores_log)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n",
    "print(\"Top items Logistic: \\n\", top_items_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "olive-maria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Van Halen' 'The Smiths' 'Siouxsie and the Banshees' ... 'Britney Spears'\n",
      " 'Rihanna' 'Miley Cyrus']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Duran Duran' 'The Beatles' ... 'Common Rotation'\n",
      " 'Dissimulation' 'Aidonia']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_bpr = model.predict(0, np.arange(n_items))\n",
    "top_items_bpr = artist_names[np.argsort(-scores_bpr)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adjustable-shannon",
   "metadata": {},
   "source": [
    "print(\"user %s\" % user_id)\n",
    "print(\"     Known positives:\")\n",
    "for x in known_positives[:3]:\n",
    "    print(\"     %s\" % x)\n",
    "\n",
    "print(\" Recommended:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "finite-ethics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fb6d1461070>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='logistic')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-seminar",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "million-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.20, test 0.07.\n",
      "AUC: train 0.89, test 0.81.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-singer",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "nonprofit-hindu",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-tamil",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adequate-assist",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Van Halen' 'The Smiths' 'Siouxsie and the Banshees' ... 'Britney Spears'\n",
      " 'Rihanna' 'Miley Cyrus']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Duran Duran' 'The Beatles' ... 'Common Rotation'\n",
      " 'Dissimulation' 'Aidonia']\n",
      "Top items Logistic: \n",
      " ['Lady Gaga' 'Britney Spears' 'Rihanna' ... 'PlatEAU' 'Deacon Blue'\n",
      " 'Преслава']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_log = model.predict(0, np.arange(n_items))\n",
    "top_items_log = artist_names[np.argsort(-scores_log)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n",
    "print(\"Top items Logistic: \\n\", top_items_log)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vocational-bearing",
   "metadata": {},
   "source": [
    "--- WARP ---\n",
    "Precision: train 0.39, test 0.14\n",
    "AUC: train 0.97, test 0.86\n",
    "    \n",
    "--- BPR ---\n",
    "Precision: train 0.36, test 0.12\n",
    "AUC: train 0.84, test 0.78\n",
    "\n",
    "--- Logistic ---\n",
    "Precision: train 0.20, test 0.07\n",
    "AUC: train 0.89, test 0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-music",
   "metadata": {},
   "source": [
    "* ### K-OS WARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "interior-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fb6d1461100>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='warp-kos')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-agriculture",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "optional-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.35, test 0.12.\n",
      "AUC: train 0.89, test 0.82.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-amber",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "formed-geneva",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-enforcement",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "scores_warp-kos = model.predict(0, np.arange(n_items))\n",
    "top_items_warp-kos = artist_names[np.argsort(-scores_warp-kos)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n",
    "print(\"Top items Logistic: \\n\", top_items_log)\n",
    "print(\"Top items k-OS Warp: \\n\", top_items_warp-kos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dutch-dayton",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Van Halen' 'The Smiths' 'Siouxsie and the Banshees' ... 'Britney Spears'\n",
      " 'Rihanna' 'Miley Cyrus']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Duran Duran' 'The Beatles' ... 'Common Rotation'\n",
      " 'Dissimulation' 'Aidonia']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_bpr = model.predict(0, np.arange(n_items))\n",
    "top_items_bpr = artist_names[np.argsort(-scores_bpr)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "running-spoke",
   "metadata": {},
   "source": [
    "print(\"user %s\" % user_id)\n",
    "print(\"     Known positives:\")\n",
    "for x in known_positives[:3]:\n",
    "    print(\"     %s\" % x)\n",
    "\n",
    "print(\" Recommended:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "maritime-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fb6d1461070>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model = LightFM(learning_rate=0.05, loss='logistic')\n",
    "model.fit(train, epochs=10, num_threads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-classic",
   "metadata": {},
   "source": [
    "Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "thick-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: train 0.20, test 0.07.\n",
      "AUC: train 0.89, test 0.81.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "# Measure the ROC AUC metric for a model: the probability that a randomly chosen positive \n",
    "# example has a higher score than a randomly chosen negative example. \n",
    "# A perfect score is 1.0.\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-appendix",
   "metadata": {},
   "source": [
    "Measure the recall at k metric for a model: the number of positive items in the first k positions of the ranked list of results divided by the number of positive items in the test period. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "raw",
   "id": "appointed-memorial",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "train_recall = recall_at_k(model, train, k=10).mean()\n",
    "test_recall = recall_at_k(model, test, k=10, train_interactions=train).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "print('Recall: train %.2f, test %.2f.' % (train_recall, test_recall))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-breed",
   "metadata": {},
   "source": [
    "Measure the reciprocal rank metric for a model: 1 / the rank of the highest ranked positive example. A perfect score is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "deluxe-blowing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top items BPR: \n",
      " ['Van Halen' 'The Smiths' 'Siouxsie and the Banshees' ... 'Britney Spears'\n",
      " 'Rihanna' 'Miley Cyrus']\n",
      "Top items WARP: \n",
      " ['Depeche Mode' 'Duran Duran' 'The Beatles' ... 'Common Rotation'\n",
      " 'Dissimulation' 'Aidonia']\n",
      "Top items Logistic: \n",
      " ['Lady Gaga' 'Britney Spears' 'Rihanna' ... 'PlatEAU' 'Deacon Blue'\n",
      " 'Преслава']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "scores_log = model.predict(0, np.arange(n_items))\n",
    "top_items_log = artist_names[np.argsort(-scores_log)]\n",
    "print(\"Top items BPR: \\n\", top_items_bpr)\n",
    "print(\"Top items WARP: \\n\", top_items_warp)\n",
    "print(\"Top items Logistic: \\n\", top_items_log)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eligible-forestry",
   "metadata": {},
   "source": [
    "--- WARP ---\n",
    "Precision: train 0.39, test 0.14\n",
    "AUC: train 0.97, test 0.86\n",
    "    \n",
    "--- BPR ---\n",
    "Precision: train 0.36, test 0.12\n",
    "AUC: train 0.84, test 0.78\n",
    "\n",
    "--- Logistic ---\n",
    "Precision: train 0.20, test 0.07\n",
    "AUC: train 0.89, test 0.81\n",
    "\n",
    "--- WARP KOS ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-illness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "imposed-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring():\n",
    "    \n",
    "    learning_rate = [0.03, 0.05, 0.08, 0.10, 0.12, 0.14]\n",
    "    losslist = ['logistic', 'bpr', 'warp', 'warp-kos']\n",
    "    klist = [3, 5, 7, 9, 11, 13]\n",
    "    results = []\n",
    "    \n",
    "    for x in learning_rate:\n",
    "        for y in losslist:\n",
    "            for z in klist:\n",
    "            \n",
    "                model = LightFM(learning_rate=x, loss = y)\n",
    "                model.fit(train, epochs=10, num_threads=2)\n",
    "\n",
    "                trainPrecision = precision_at_k(model, train, k=z).mean()\n",
    "                testPrecision = precision_at_k(model, test, k=z, train_interactions=train).mean()\n",
    "\n",
    "                trainAUC = auc_score(model, train).mean()\n",
    "                testAUC = auc_score(model, test, train_interactions=train).mean()\n",
    "\n",
    "                dicttemp = {}\n",
    "                dicttemp = {'K':z, 'Name':y, 'Learning Rate':x, 'Train Precision':trainPrecision, 'Train AUC':trainAUC, 'Test Precision':testPrecision, \"Train AUC\":trainAUC, \"Test AUC\":testAUC}\n",
    "\n",
    "                results.append(dicttemp)\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "economic-directory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Name</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Train AUC</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.229421</td>\n",
       "      <td>0.886406</td>\n",
       "      <td>0.089244</td>\n",
       "      <td>0.807125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.217313</td>\n",
       "      <td>0.886890</td>\n",
       "      <td>0.080747</td>\n",
       "      <td>0.806915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.207799</td>\n",
       "      <td>0.887001</td>\n",
       "      <td>0.073752</td>\n",
       "      <td>0.807343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.199622</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.069748</td>\n",
       "      <td>0.807469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.195095</td>\n",
       "      <td>0.886657</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.807637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5</td>\n",
       "      <td>warp-kos</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.277005</td>\n",
       "      <td>0.878168</td>\n",
       "      <td>0.095147</td>\n",
       "      <td>0.794324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7</td>\n",
       "      <td>warp-kos</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.300281</td>\n",
       "      <td>0.878412</td>\n",
       "      <td>0.100571</td>\n",
       "      <td>0.793915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>9</td>\n",
       "      <td>warp-kos</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>0.879012</td>\n",
       "      <td>0.094519</td>\n",
       "      <td>0.794209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>11</td>\n",
       "      <td>warp-kos</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.288756</td>\n",
       "      <td>0.879915</td>\n",
       "      <td>0.088533</td>\n",
       "      <td>0.798933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13</td>\n",
       "      <td>warp-kos</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.293558</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.794816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      K      Name  Learning Rate  Train Precision  Train AUC  Test Precision  \\\n",
       "0     3  logistic           0.03         0.229421   0.886406        0.089244   \n",
       "1     5  logistic           0.03         0.217313   0.886890        0.080747   \n",
       "2     7  logistic           0.03         0.207799   0.887001        0.073752   \n",
       "3     9  logistic           0.03         0.199622   0.886737        0.069748   \n",
       "4    11  logistic           0.03         0.195095   0.886657        0.068800   \n",
       "..   ..       ...            ...              ...        ...             ...   \n",
       "139   5  warp-kos           0.14         0.277005   0.878168        0.095147   \n",
       "140   7  warp-kos           0.14         0.300281   0.878412        0.100571   \n",
       "141   9  warp-kos           0.14         0.296926   0.879012        0.094519   \n",
       "142  11  warp-kos           0.14         0.288756   0.879915        0.088533   \n",
       "143  13  warp-kos           0.14         0.293558   0.879400        0.088000   \n",
       "\n",
       "     Test AUC  \n",
       "0    0.807125  \n",
       "1    0.806915  \n",
       "2    0.807343  \n",
       "3    0.807469  \n",
       "4    0.807637  \n",
       "..        ...  \n",
       "139  0.794324  \n",
       "140  0.793915  \n",
       "141  0.794209  \n",
       "142  0.798933  \n",
       "143  0.794816  \n",
       "\n",
       "[144 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-mineral",
   "metadata": {},
   "source": [
    "## The best \"Test AUC\" score (85,70%) is done with a \"learning rate\": 5% and \"k\"=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "A continuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-hanging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
